services:

  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    restart: unless-stopped
    entrypoint: /bin/sh -c "ollama serve & sleep 3 && ollama pull mistral && wait"
#    entrypoint: /bin/sh -c "ollama serve & sleep 3 && ollama pull llama3:8b-instruct-q4_K_M && wait"
#    entrypoint: /bin/sh -c "ollama serve & sleep 3 && ollama pull llama3:8b && wait"

#  mcp-server:
#      build:
#        context: ./mcp_server
#      ports:
#        - "8000:8000"
#      restart: always
#
#  agent:
#    build:
#      context: ./agent
#    depends_on:
#      - ollama
#      - mcp-server
#    command: ["python", "main.py"]
#    environment:
#      - OLLAMA_BASE_URL=http://ollama:11434
#      - MCP_SERVER_URL=http://mcp-server:8000

volumes:
  ollama: